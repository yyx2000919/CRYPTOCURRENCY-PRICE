{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa75f5c3",
   "metadata": {},
   "source": [
    "# Define the model\n",
    "## define RNN LSTM GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff98f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: UTF-8 -*-\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.nn import Module, LSTM, Linear, RNN, GRU\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "\n",
    "class Net(Module):\n",
    "    \n",
    "    def __init__(self,config):\n",
    "        super(Net, self).__init__()\n",
    "        if config.model_type =='LSTM':\n",
    "            self.rnn = LSTM(input_size=config.input_size, hidden_size=config.hidden_size,\n",
    "                            num_layers=config.lstm_layers, batch_first=True, dropout=config.dropout_rate) #set batch_first to True here to make the first dimension become batch.\n",
    "        elif config.model_type =='RNN':\n",
    "            self.rnn = RNN(input_size=config.input_size, hidden_size=config.hidden_size,\n",
    "                            num_layers=config.lstm_layers, batch_first=True, dropout=config.dropout_rate)\n",
    "        elif config.model_type =='GRU':\n",
    "            \n",
    "            self.rnn = GRU(input_size=config.input_size, hidden_size=config.hidden_size,                   #Since all three neural networks have the same number of layers, \n",
    "                            num_layers=config.lstm_layers, batch_first=True, dropout=config.dropout_rate)  # their num_layer is lstm_layers, \n",
    "        self.linear = Linear(in_features=config.hidden_size, out_features=config.output_size)              # and they only need to be defined once after that.\n",
    "\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        #net_out, hidden = self.lstm(x, hidden)\n",
    "        #net_out, hidden = self.rnn(x, hidden)\n",
    "        net_out, hidden = self.rnn(x, hidden)\n",
    "        linear_out = self.linear(net_out)\n",
    "        return linear_out, hidden\n",
    "\n",
    "\n",
    "def train(config, logger, train_and_valid_data):\n",
    "    if config.do_train_visualized:\n",
    "        import visdom\n",
    "        vis = visdom.Visdom(env='model_pytorch')\n",
    "\n",
    "    train_X, train_Y, valid_X, valid_Y = train_and_valid_data\n",
    "    train_X, train_Y = torch.from_numpy(train_X).float(), torch.from_numpy(train_Y).float()     # turn to tensor first\n",
    "    train_loader = DataLoader(TensorDataset(train_X, train_Y), batch_size=config.batch_size)    # DataLoader can automatically generate trainable batch data\n",
    "\n",
    "    valid_X, valid_Y = torch.from_numpy(valid_X).float(), torch.from_numpy(valid_Y).float()\n",
    "    valid_loader = DataLoader(TensorDataset(valid_X, valid_Y), batch_size=config.batch_size)\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if config.use_cuda and torch.cuda.is_available() else \"cpu\") # CPU or GPU\n",
    "    model = Net(config).to(device)      # If it is GPU training, .to(device) will copy the model/data to GPU memory\n",
    "    if config.add_train:                # If it is incremental training, the original model parameters will be loaded first\n",
    "        model.load_state_dict(torch.load(config.model_save_path + config.model_name))\n",
    "    #TODO\n",
    "    if config.optimizer == 'SGD':\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=config.learning_rate)\n",
    "    else:\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "    criterion = torch.nn.MSELoss()      # define optimizer and loss function\n",
    "\n",
    "    valid_loss_min = float(\"inf\")\n",
    "    bad_epoch = 0\n",
    "    global_step = 0\n",
    "    step_count = 0\n",
    "    losses = []\n",
    "    ##train for epochs\n",
    "    for epoch in range(config.epoch):\n",
    "        logger.info(\"Epoch {}/{}\".format(epoch, config.epoch))\n",
    "        model.train()                   \n",
    "        train_loss_array = []\n",
    "        hidden_train = None\n",
    "        for i, _data in enumerate(train_loader):\n",
    "            _train_X, _train_Y = _data[0].to(device),_data[1].to(device)\n",
    "            optimizer.zero_grad()           \n",
    "            pred_Y, hidden_train = model(_train_X, hidden_train)    # forward calculation \n",
    "\n",
    "            if not config.do_continue_train:\n",
    "                hidden_train = None            \n",
    "            else:\n",
    "                h_0, c_0 = hidden_train\n",
    "                h_0.detach_(), c_0.detach_()    # remove gradient information\n",
    "                hidden_train = (h_0, c_0)\n",
    "            \n",
    "                \n",
    "            loss = criterion(pred_Y, _train_Y)  # Calculate loss\n",
    "            if step_count % 500 ==0:\n",
    "                losses.append(loss.item())\n",
    "            loss.backward()                     # Backpropagating the loss\n",
    "            step_count+=1\n",
    "            optimizer.step()                    # Update parameters with optimizer\n",
    "            train_loss_array.append(loss.item())\n",
    "            global_step += 1\n",
    "            if config.do_train_visualized and global_step % 100 == 0:   # Displayed every hundred steps\n",
    "                vis.line(X=np.array([global_step]), Y=np.array([loss.item()]), win='Train_Loss',\n",
    "                         update='append' if global_step > 0 else None, name='Train', opts=dict(showlegend=True))\n",
    "\n",
    "        # The following is the early stop mechanism. \n",
    "        # When the model training does not improve the prediction effect of the validation set for consecutive config.patience epochs, \n",
    "        # it stops to prevent overfitting.\n",
    "        model.eval()                    \n",
    "        valid_loss_array = []\n",
    "        hidden_valid = None\n",
    "        for _valid_X, _valid_Y in valid_loader:\n",
    "            _valid_X, _valid_Y = _valid_X.to(device), _valid_Y.to(device)\n",
    "            pred_Y, hidden_valid = model(_valid_X, hidden_valid)\n",
    "            if not config.do_continue_train: hidden_valid = None\n",
    "            loss = criterion(pred_Y, _valid_Y)  # The verification process only has forward calculation, no back propagation process\n",
    "            valid_loss_array.append(loss.item())\n",
    "\n",
    "        train_loss_cur = np.mean(train_loss_array)\n",
    "        valid_loss_cur = np.mean(valid_loss_array)\n",
    "        logger.info(\"The train loss is {:.6f}. \".format(train_loss_cur) +\n",
    "              \"The valid loss is {:.6f}.\".format(valid_loss_cur))\n",
    "        if config.do_train_visualized:      \n",
    "            vis.line(X=np.array([epoch]), Y=np.array([train_loss_cur]), win='Epoch_Loss',\n",
    "                     update='append' if epoch > 0 else None, name='Train', opts=dict(showlegend=True))\n",
    "            vis.line(X=np.array([epoch]), Y=np.array([valid_loss_cur]), win='Epoch_Loss',\n",
    "                     update='append' if epoch > 0 else None, name='Eval', opts=dict(showlegend=True))\n",
    "\n",
    "        if valid_loss_cur < valid_loss_min:\n",
    "            valid_loss_min = valid_loss_cur\n",
    "            bad_epoch = 0\n",
    "            torch.save(model.state_dict(), config.model_save_path + config.model_name)  # save the model\n",
    "        else:\n",
    "            bad_epoch += 1\n",
    "            if bad_epoch >= config.patience:\n",
    "                #If the validation set indicators do not improve for consecutive patient epochs, stop training\n",
    "                logger.info(\" The training stops early in epoch {}\".format(epoch))\n",
    "   \n",
    "                break\n",
    "    plt.plot(losses,label = 'loss')\n",
    "    plt.legend(loc=0)\n",
    "    plt.show()\n",
    "    ##predict the test data\n",
    "def predict(config, test_X):\n",
    "    # Get test data\n",
    "    test_X = torch.from_numpy(test_X).float()\n",
    "    test_set = TensorDataset(test_X)\n",
    "    test_loader = DataLoader(test_set, batch_size=1)\n",
    "\n",
    "    # load the model\n",
    "    device = torch.device(\"cuda:0\" if config.use_cuda and torch.cuda.is_available() else \"cpu\")\n",
    "    model = Net(config).to(device)\n",
    "    model.load_state_dict(torch.load(config.model_save_path + config.model_name))   #Loading model parameters\n",
    "\n",
    "    # First define a tensor to save the prediction results\n",
    "    result = torch.Tensor().to(device)\n",
    "\n",
    "    model.eval()\n",
    "    hidden_predict = None\n",
    "    for _data in test_loader:\n",
    "        data_X = _data[0].to(device)\n",
    "        pred_X, hidden_predict = model(data_X, hidden_predict)\n",
    "        # The experiment found that whether it is continuous training mode or not, \n",
    "        #it is better to pass the hidden of the previous time_step into the next one.\n",
    "        cur_pred = torch.squeeze(pred_X, dim=0)\n",
    "        result = torch.cat((result, cur_pred), dim=0)\n",
    "\n",
    "    return result.detach().cpu().numpy()   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c51002",
   "metadata": {},
   "source": [
    "# setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fe474b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: UTF-8 -*-\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "from logging.handlers import RotatingFileHandler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        #self.label_columns = label\n",
    "        #self.predict_day = pre\n",
    "\n",
    "        self.feature_columns = list([1,2,3,4])  #choose features: midpoint、spread、buys、sells\n",
    "        self.label_columns = [1]                  #label：midpoint\n",
    "        # label_in_feature_index = [feature_columns.index(i) for i in label_columns] \n",
    "        self.label_in_feature_index = (lambda x, y: [x.index(i) for i in y])(self.feature_columns,self.label_columns)  \n",
    "        self.model_type = 'LSTM'         #LSTM or RNN or GRU\n",
    "        # TODO\n",
    "        self.predict_day = 5             # input the number of moments want to predict next\n",
    "        self.optimizer = 'SGD'           # SGD or Adam\n",
    "        # set network parameters\n",
    "        self.input_size = len(self.feature_columns)\n",
    "        self.output_size = len(self.label_columns)\n",
    "\n",
    "        self.hidden_size = 128  # \n",
    "        self.lstm_layers = 2  #\n",
    "        self.dropout_rate = 0.2  # dropout rate\n",
    "        self.time_step = 60  # This parameter is very important. It is used to set the number of previous data to predict, \n",
    "        #and it is also the number of time steps of LSTM. \n",
    "        #Please ensure that the amount of training data is larger than it.\n",
    "\n",
    "        # set training parameters\n",
    "        self.do_train = True\n",
    "        self.do_predict = True                  # whether to predict next moments\n",
    "        self.add_train = False                  # whether to load existing model parameters for incremental training\n",
    "        self.shuffle_train_data = True          # whether to shuffle train data\n",
    "        self.use_cuda = False                   \n",
    "        self.train_data_rate = 0.7              #train dataset: 70% of data\n",
    "        self.valid_data_rate = 0.1              #valid dataset: 10% of data\n",
    "        self.batch_size = 64\n",
    "        self.learning_rate = 3e-4   #0.001\n",
    "        self.epoch = 20                         # epochs of training\n",
    "        self.patience = 5                       #stop if the validation set is not improved in 5 epochs\n",
    "        self.random_seed = 42                   #random seed \n",
    "\n",
    "        self.do_continue_train = False  #\n",
    "        self.continue_flag = \"\"  #\n",
    "        if self.do_continue_train:\n",
    "            self.shuffle_train_data = False\n",
    "            self.batch_size = 1\n",
    "            self.continue_flag = \"continue_\"\n",
    "\n",
    "        # training mode\n",
    "        self.debug_mode = False     #whether to debug\n",
    "        self.debug_num = 500  \n",
    "\n",
    "        # framework: pytorch\n",
    "        self.used_frame = 'pytorch'  \n",
    "        model_postfix = {\"pytorch\": \".pth\", \"keras\": \".h5\", \"tensorflow\": \".ckpt\"}\n",
    "        self.model_name = \"model_\" + self.continue_flag + self.used_frame + model_postfix[self.used_frame]\n",
    "\n",
    "        # set path parameters\n",
    "        # TODO\n",
    "        self.train_data_path = \"./data/btc_data.csv\"\n",
    "        self.model_save_path = \"./checkpoint/\" + self.used_frame + \"/\"\n",
    "        self.figure_save_path = \"./figure/\"\n",
    "        self.log_save_path = \"./log/\"\n",
    "        self.do_log_print_to_screen = True\n",
    "        self.do_log_save_to_file = True  #\n",
    "        # TODO\n",
    "        self.do_figure_save = True\n",
    "        self.do_train_visualized = False  # \n",
    "        if not os.path.exists(self.model_save_path):\n",
    "            os.makedirs(self.model_save_path)  # \n",
    "        if not os.path.exists(self.figure_save_path):\n",
    "            os.mkdir(self.figure_save_path)\n",
    "        if self.do_train and (self.do_log_save_to_file or self.do_train_visualized):\n",
    "            cur_time = time.strftime(\"%Y_%m_%d_%H_%M_%S\", time.localtime())\n",
    "            log_save_path = self.log_save_path + cur_time + '_' + self.used_frame + \"/\"\n",
    "            os.makedirs(log_save_path)\n",
    "\n",
    "\n",
    "class Data:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.data, self.data_column_name = self.read_data()\n",
    "\n",
    "\n",
    "        self.data_num = self.data.shape[0]\n",
    "        self.train_num = int(self.data_num * self.config.train_data_rate)\n",
    "\n",
    "        self.mean = np.mean(self.data, axis=0)              # evaluate the mean of the data\n",
    "        self.std = np.std(self.data, axis=0)                #evaluate the variance of the data\n",
    "        self.norm_data = (self.data - self.mean)/self.std  \n",
    "\n",
    "        self.start_num_in_test = 0      \n",
    "\n",
    "    def read_data(self):                # read data from csv file\n",
    "        if self.config.debug_mode:\n",
    "            init_data = pd.read_csv(self.config.train_data_path, nrows=self.config.debug_num,\n",
    "                                    usecols=self.config.feature_columns)\n",
    "        else:\n",
    "            init_data = pd.read_csv(self.config.train_data_path, usecols=self.config.feature_columns)\n",
    "        return init_data.values, init_data.columns.tolist()     # .columns.tolist() to get the name of columns\n",
    "\n",
    "    def get_train_and_valid_data(self):\n",
    "        feature_data = self.norm_data[:self.train_num]\n",
    "        label_data = self.norm_data[self.config.predict_day : self.config.predict_day + self.train_num,\n",
    "                                    self.config.label_in_feature_index]    \n",
    "\n",
    "        if not self.config.do_continue_train:\n",
    "            #\n",
    "            train_x = [feature_data[i:i+self.config.time_step] for i in range(self.train_num-self.config.time_step)]\n",
    "            train_y = [label_data[i:i+self.config.time_step] for i in range(self.train_num-self.config.time_step)]\n",
    "        else:\n",
    "            # In continuous training mode, \n",
    "            #each time_step row of data will be used as a sample, and the two samples are staggered by the time_step row.\n",
    "            train_x = [feature_data[start_index + i*self.config.time_step : start_index + (i+1)*self.config.time_step]\n",
    "                       for start_index in range(self.config.time_step)\n",
    "                       for i in range((self.train_num - start_index) // self.config.time_step)]\n",
    "            train_y = [label_data[start_index + i*self.config.time_step : start_index + (i+1)*self.config.time_step]\n",
    "                       for start_index in range(self.config.time_step)\n",
    "                       for i in range((self.train_num - start_index) // self.config.time_step)]\n",
    "\n",
    "        train_x, train_y = np.array(train_x), np.array(train_y)\n",
    "\n",
    "        train_x, valid_x, train_y, valid_y = train_test_split(train_x, train_y, test_size=self.config.valid_data_rate,\n",
    "                                                              random_state=self.config.random_seed,\n",
    "                                                              shuffle=self.config.shuffle_train_data)   # Divide training and validation sets, and shuffle\n",
    "        return train_x, valid_x, train_y, valid_y\n",
    "\n",
    "    def get_test_data(self, return_label_data=False):\n",
    "        feature_data = self.norm_data[self.train_num:]\n",
    "        sample_interval = min(feature_data.shape[0], self.config.time_step)     \n",
    "        self.start_num_in_test = feature_data.shape[0] % sample_interval  \n",
    "        time_step_size = feature_data.shape[0] // sample_interval\n",
    "\n",
    "        # In the test data, each time_step row of data will be used as a sample, \n",
    "        #and the two samples are staggered by the time_step row\n",
    "        test_x = [feature_data[self.start_num_in_test+i*sample_interval : self.start_num_in_test+(i+1)*sample_interval]\n",
    "                   for i in range(time_step_size)]\n",
    "        if return_label_data:       \n",
    "            label_data = self.norm_data[self.train_num + self.start_num_in_test:, self.config.label_in_feature_index]\n",
    "            return np.array(test_x), label_data\n",
    "        return np.array(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7380dc7f",
   "metadata": {},
   "source": [
    "# Defining drawing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075ea192",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_logger(config):\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(level=logging.DEBUG)\n",
    "\n",
    "    # StreamHandler\n",
    "    if config.do_log_print_to_screen:\n",
    "        stream_handler = logging.StreamHandler(sys.stdout)\n",
    "        stream_handler.setLevel(level=logging.INFO)\n",
    "        formatter = logging.Formatter(datefmt='%Y/%m/%d %H:%M:%S',\n",
    "                                      fmt='[ %(asctime)s ] %(message)s')\n",
    "        stream_handler.setFormatter(formatter)\n",
    "        logger.addHandler(stream_handler)\n",
    "\n",
    "    # FileHandler\n",
    "    if config.do_log_save_to_file:\n",
    "        file_handler = RotatingFileHandler(config.log_save_path + \"out.log\", maxBytes=1024000, backupCount=5)\n",
    "        file_handler.setLevel(level=logging.INFO)\n",
    "        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "        file_handler.setFormatter(formatter)\n",
    "        logger.addHandler(file_handler)\n",
    "\n",
    "        # Also record the config information to the log file\n",
    "        config_dict = {}\n",
    "        for key in dir(config):\n",
    "            if not key.startswith(\"_\"):\n",
    "                config_dict[key] = getattr(config, key)\n",
    "        config_str = str(config_dict)\n",
    "        config_list = config_str[1:-1].split(\", '\")\n",
    "        config_save_str = \"\\nConfig:\\n\" + \"\\n'\".join(config_list)\n",
    "        logger.info(config_save_str)\n",
    "\n",
    "    return logger\n",
    "\n",
    "def draw(config: Config, origin_data: Data, logger, predict_norm_data: np.ndarray):\n",
    "    label_data = origin_data.data[origin_data.train_num + origin_data.start_num_in_test : ,\n",
    "                                            config.label_in_feature_index]\n",
    "    predict_data = predict_norm_data * origin_data.std[config.label_in_feature_index] + \\\n",
    "                   origin_data.mean[config.label_in_feature_index]   # Restore data with saved mean and variance\n",
    "    assert label_data.shape[0]==predict_data.shape[0], \"The element number in origin and predicted data is different\"\n",
    "\n",
    "    label_name = [origin_data.data_column_name[i] for i in config.label_in_feature_index]\n",
    "    label_column_num = len(config.label_columns)\n",
    "\n",
    "    loss = np.mean((label_data[config.predict_day:] - predict_data[:-config.predict_day] ) ** 2, axis=0)\n",
    "    loss_norm = loss/(origin_data.std[config.label_in_feature_index] ** 2)\n",
    "    logger.info(\"The mean squared error of btc {} is \".format(label_name) + str(loss_norm))\n",
    "\n",
    "    label_X = range(origin_data.data_num - origin_data.train_num - origin_data.start_num_in_test)\n",
    "    predict_X = [ x + config.predict_day for x in label_X]\n",
    "\n",
    "    res = {}\n",
    "    for i in range(int(config.predict_day)):\n",
    "        if label_name == ['low','high']:\n",
    "            res[i] = dict(low=np.squeeze(predict_data[-config.predict_day:, 0][i]),high=np.squeeze(predict_data[-config.predict_day:, 1][i]))\n",
    "        elif label_name == ['low']:\n",
    "            res[i] = dict(low=np.squeeze(predict_data[-config.predict_day:, 0][i]))\n",
    "        elif label_name == ['high']:\n",
    "            res[i] = dict(high=np.squeeze(predict_data[-config.predict_day:, 0][i]))\n",
    "        elif label_name == ['midpoint']:\n",
    "            res[i] = dict(mid=np.squeeze(predict_data[-config.predict_day:, 0][i]))\n",
    "    #print(res)\n",
    "\n",
    "    if not sys.platform.startswith('linux'):    # The output cannot be output under Linux without a desktop.\n",
    "        for i in range(label_column_num):\n",
    "            #print(i)\n",
    "            plt.figure(i+1)                     # Forecast data plot\n",
    "            plt.plot(label_X, label_data[:, i], label='label',color = 'b')\n",
    "            plt.plot(predict_X, predict_data[:, i], label='predict',color = 'r')\n",
    "            plt.title(\"Predict btc {} price with {}\".format(label_name[i], config.used_frame))\n",
    "            logger.info(\"The predicted btc {} for the next {} day(s) is: \".format(label_name[i], config.predict_day) +\n",
    "                  str(np.squeeze(predict_data[-config.predict_day:, i])))\n",
    "            if config.do_figure_save:\n",
    "                plt.savefig(config.figure_save_path+\"{}predict_{}_with_{}.png\".format(config.continue_flag, label_name[i], config.used_frame))\n",
    "        plt.legend(loc=0)\n",
    "        plt.show()\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea1c32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(config):\n",
    "    logger = load_logger(config)   #load config\n",
    "    np.random.seed(config.random_seed)  # Set a random seed to ensure reproducibility\n",
    "    data_gainer = Data(config)\n",
    "\n",
    "    if config.do_train:\n",
    "        train_X, valid_X, train_Y, valid_Y = data_gainer.get_train_and_valid_data()   #get train data\n",
    "        train(config, logger, [train_X, train_Y, valid_X, valid_Y])                   #train the data\n",
    "\n",
    "    if config.do_predict:\n",
    "        test_X, test_Y = data_gainer.get_test_data(return_label_data=True)          #get test data\n",
    "        pred_result = predict(config, test_X)       # The output here is the unrestored normalized prediction data\n",
    "        out_dict = draw(config, data_gainer, logger, pred_result)                   #draw the picture\n",
    "\n",
    "    return out_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754f5ec6",
   "metadata": {},
   "source": [
    "# Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980a6760",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "con = Config()\n",
    "\n",
    "con.model_type ='RNN'\n",
    "con.optimizer = 'SGD'\n",
    "out = main(con)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedb0be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "con = Config()\n",
    "\n",
    "con.model_type ='RNN'\n",
    "con.optimizer = 'Adam'\n",
    "out = main(con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f45a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "con = Config()\n",
    "\n",
    "con.model_type ='LSTM'\n",
    "con.optimizer = 'Adam'\n",
    "out = main(con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e115b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "con = Config()\n",
    "\n",
    "con.model_type ='LSTM'\n",
    "con.optimizer = 'SGD'\n",
    "out = main(con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6dbf71",
   "metadata": {},
   "outputs": [],
   "source": [
    "con = Config()\n",
    "\n",
    "con.model_type ='GRU'\n",
    "con.optimizer = 'SGD'\n",
    "out = main(con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b668aaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "con = Config()\n",
    "\n",
    "con.model_type ='GRU'\n",
    "con.optimizer = 'Adam'\n",
    "out = main(con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd53655",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08eb28d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "b822e130f92d93f1371ffe29eae288679c8a93cb14180dc721f4236906935282"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
